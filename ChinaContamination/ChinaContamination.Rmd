---
title: "China Contamination"
subtitle: "Data Science: Capstone Proyect"
author: "Jorge Haces"
date: "16/6/2020"
output: 
    pdf_document:
     toc: true
     toc_depth: 3
     number_sections: true
header-includes:
- \usepackage{float}

---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.pos = 'H')
knitr::opts_chunk$set(fig.align = 'center')

Sys.setlocale("LC_ALL","English")
```
\newpage

# Overview
This is the final project required for **Data Science: Capstone** course offered by *edX HarvardX for Professional Certificate Program in Data Science*. The theme of this proyect is the Contamination, specifically in China and the aim is to predict the air quality in the fastest growing country nowadays.


## Introduction
Contanimation is defined as the presence of materials in the air that cause serious harm or discomfort to people. The contamination has increased since the Industrial Revolution began, in the second half of the 18th century, with production processes in factories, the development of transportation and the use of fuels.
 
According to the World Health Organization (WHO), the state of the current atmosphere causes, by the simple act of breathing, the death of around seven million people a year (fine particle respiration).

The most common air pollutant gases are carbon monoxide, sulfur dioxide, chlorofluorocarbons, and nitrogen oxides.
Photochemicals such as ozone and smog are increased in the air by nitrogen oxides and hydrocarbons reacting with sunlight.

Contaminants are classified into:

• Primaries are those that are emitted directly into the atmosphere such as sulfur dioxide, carbon monoxide

• Secondary are those that are formed by atmospheric chemical processes that act on primary contaminants such as sulfuric acid, which is formed by the oxidation of sulfur dioxide, nitrogen dioxide that is formed by oxidizing the primary pollutant nitric oxide and ozone that is formed from oxygen.
[[1](https://es.wikipedia.org/wiki/Contaminaci%C3%B3n_atmosf%C3%A9rica)]


## Project Description

An analysis of the data will be carried out based on the following models:  k Nearest Neighbors, Logistic Regression, Support Vector Machines (SVM), Random Forests and Neural Network to help us predict if pollution will grow even more (2.3% in 2018 almost at double compared to 2010). To answer the question, will china be able to comply with the Paris agreement signed in 2015? Whose goal is to reduce the global temperature to 2°C in 2050.

For this we will divide the data into two: training data and test data. Later, we will train the different models in the first set and then will be evaluated in the second set.
Finally, we will use the **Root-Mean-Square-Error (RMSE)** and the **“overall accuracy”** to rate the performance of each model and thus identify the best for this project.

\newpage
## DataSet 
The Dataset used in this project is *Beijing Multi-Site Air-Quality Data Data Set*, available at the UCI Machine Learning Repository [[2](https://archive.ics.uci.edu/ml/datasets/Beijing+Multi-Site+Air-Quality+Data)].

This data set includes hourly air pollutants data from 12 nationally-controlled air-quality monitoring sites. The air-quality data are from the Beijing Municipal Environmental Monitoring Center. The meteorological data in each air-quality site are matched with the nearest weather station from the China Meteorological Administration. The time period is from March 1st, 2013 to February 28th, 2017. Missing data are denoted as NA.


The Attribute Information is the following:

* No: row number
* year: year of data in this row
* month: month of data in this row
* day: day of data in this row
* hour: hour of data in this row
* PM2.5: PM2.5 concentration (ug/m^3)
* PM10: PM10 concentration (ug/m^3)
* SO2: SO2 concentration (ug/m^3)
* NO2: NO2 concentration (ug/m^3)
* CO: CO concentration (ug/m^3)
* O3: O3 concentration (ug/m^3)
* TEMP: temperature (degree Celsius)
* PRES: pressure (hPa)
* DEWP: dew point temperature (degree Celsius)
* RAIN: precipitation (mm)
* wd: wind direction
* WSPM: wind speed (m/s)
* station: name of the air-quality monitoring site

The data is contained in a Zip file named *PRSA2017_Data_20130301-20170228.zip* containing 12 files (
one for each municipality), as follow:

* PRSA_Data_Aotizhongxin_20130301-20170228.csv
* PRSA_Data_Changping_20130301-20170228.csv
* PRSA_Data_Dingling_20130301-20170228.csv
* PRSA_Data_Dongsi_20130301-20170228.csv
* PRSA_Data_Guanyuan_20130301-20170228.csv
* PRSA_Data_Gucheng_20130301-20170228.csv
* PRSA_Data_Huairou_20130301-20170228.csv
* PRSA_Data_Nongzhanguan_20130301-20170228.csv
* PRSA_Data_Shunyi_20130301-20170228.csv
* PRSA_Data_Tiantan_20130301-20170228.csv
* PRSA_Data_Wanliu_20130301-20170228.csv
* PRSA_Data_Wanshouxigong_20130301-20170228.csv

\newpage

# Methods and Analysis

## Data Stage 

Next, the UCI data will be downloaded in ZIP format to decompress them and load the 12 files in a single variable called PRSA, identifying the data to be analyzed with 420,768 records and 18 attributes (columns).

```{r Environment  Settings, message=FALSE, warning=FALSE, echo=FALSE, results="hide"}

#### Environment  Settings

# Packages required for this proyect
pkgs <-c("tidyverse",   "caret","kernlab",  "randomForest","knitr")

# If a package is missing it will added in the missing packages list
missing_pkgs <-pkgs[!(pkgs %in% installed.packages())]

# The packages in the list will be installed
if (length(missing_pkgs)) {
  install.packages(missing_pkgs, repos = "http://cran.rstudio.com")
}
#Load the required libraries
library(caret)
library(tidyverse)
library(kernlab)
library(randomForest)
library(knitr)
```
```{r DataLoad, message=FALSE, warning=FALSE, echo=FALSE, error=FALSE}

#Download ZIP file with data
if (!file.exists("PRSA2017_Data_20130301-20170228.zip")){
  download.file("https://archive.ics.uci.edu/ml/machine-learning-databases/00501/PRSA2017_Data_20130301-20170228.zip", "PRSA2017_Data_20130301-20170228.zip")
}
# Unzip the data file
if (!file.exists("Data")){
zip_file<- ".\\PRSA2017_Data_20130301-20170228.zip"
outDir<-"Data"
unzip(zip_file,exdir=outDir)
}
#Read the 12 csv files into a variable
PRSA <- list.files(path = ".\\Data\\PRSA_Data_20130301-20170228",pattern = "*.csv", full.names = TRUE) %>% lapply(read_csv) %>% bind_rows                                                        
 
# Dimensions of the original data (rows,columns) 
dim(PRSA)
```

Subsequently, we execute the nearZeroVar function to identify the attributes that have no significant variation in our data set; In this case, the attribute "Rain" does not present significant variations, so we remove that attribute. 

```{r DataStage_PRSA, message=FALSE, warning=FALSE, echo=TRUE, error=FALSE}
# Structure of the data (data type, numbers of rows, number of attributes)
str(PRSA)

#Near zero variance ( identify the attributes that do not give us valuable data )
nzv <- nearZeroVar(PRSA)

#Remove the nzv columns and save in another variable
data <- PRSA[,-nzv]

```

Therefore, we will continue with 17 attributes.

```{r DataStage_Ftype, message=FALSE, warning=FALSE, echo=TRUE, error=FALSE}
# Dimensions of the valuable data (rows,columns) 
dim(data)
```
It was identified that the attribute "No" is a consecutive attribute and not contribute anything to the data, therefore it will also be discarded. It is identified that all the attributes are numerical type with the exception of “station” which is character type, therefore, the attributes will be transformed to factor type to make the data set more efficient.

```{r DataStage_Ftype2, message=FALSE, warning=FALSE, echo=TRUE, error=FALSE}
# Show the data in a transposed version to see more data
glimpse(data)

#Remove the "no" variable because is a consecutive (also do not give us valuable data)
data <- data[,-c(1)]

# Cast the attributes year, month, day, hour, wd and station from "dbl" type to "factor" type
data$year <- as.factor(data$year) 
data$month <- as.factor(data$month) 
data$day <- as.factor(data$day) 
data$hour <- as.factor(data$hour) 
data$wd <- as.factor(data$wd) 
data$station <- as.factor(data$station) 

```

Similarly, it is validated that the PM (2.5 and 10) attributes; in this case, only the attribute “CO” will be transformed to an integer.

```{r DataStage_Itype, message=FALSE, warning=FALSE, echo=FALSE, error=FALSE}
# Analyse if the the attributes PM (2.5 and 10) and CO can be cast to Integer
unique(data$PM2.5)
unique(data$PM10)
unique(data$CO)

# Only the attribute CO can be casted to integer
data$CO <- as.integer(data$CO) 

```
```{r DataStage_ItypeCO, message=FALSE, warning=FALSE, echo=TRUE, error=FALSE}
# Review the first 6 rows
head(data)

```

We visualize the data set with the changes made

```{r DataStage_Review, message=FALSE, warning=FALSE, echo=TRUE, error=FALSE}
# Review the previous changes in a transposed version to see more data
glimpse(data)

```


Finally, we run the display of the attribute statistics. It is important to note that the mean and  median values, in every attribute, are not very far from each other, therefore, there are not many scattered data.

```{r DataStage_Summary, message=FALSE, warning=FALSE, echo=TRUE, error=FALSE}
# Review the statistics of each attribute
summary(data)

```

We can verify in the following histogram in this case with the attribute CO

```{r DataStage_histogram, message=FALSE, warning=FALSE, echo=TRUE, error=FALSE}
# Histogram of Carbon Monoxide
data %>%
  ggplot(aes(CO)) + geom_histogram() +
  labs(title = "China Contamination", x = "Carbon Monoxide")

```

